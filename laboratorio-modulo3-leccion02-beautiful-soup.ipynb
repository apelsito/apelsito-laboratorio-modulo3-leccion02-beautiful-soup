{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/Hack-io-Data/Imagenes/blob/main/01-LogosHackio/logo_naranja@4x.png?raw=true\" alt=\"esquema\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "º## Contexto\n",
    "\n",
    "**Descripción:**\n",
    "\n",
    "SetMagic Productions es una empresa especializada en la provisión de servicios integrales para la realización de rodajes cinematográficos y audiovisuales. Nos dedicamos a facilitar tanto el atrezzo necesario para las producciones como los lugares idóneos para llevar a cabo los rodajes, ya sea en entornos al aire libre o en interiores.\n",
    "\n",
    "**Servicios Ofrecidos:**\n",
    "\n",
    "- **Atrezzo Creativo:** Contamos con un extenso catálogo de atrezzo que abarca desde accesorios hasta muebles y objetos temáticos para ambientar cualquier tipo de  escena.\n",
    "\n",
    "- **Locaciones Únicas:** Nuestra empresa ofrece una amplia selección de locaciones, que incluyen desde escenarios naturales como playas, bosques y montañas, hasta espacios interiores como estudios, casas históricas y edificios emblemáticos.\n",
    "- **Servicios de Producción:** Además de proporcionar atrezzo y locaciones, también ofrecemos servicios de producción audiovisual, incluyendo equipos de filmación, personal técnico y servicios de postproducción.\n",
    "\n",
    "**Herramientas y Tecnologías:**\n",
    "\n",
    "Para recopilar información sobre nuevas locaciones y tendencias en atrezzo, utilizamos herramientas de web scraping como Beautiful Soup y Selenium para extraer datos de sitios web relevantes y redes sociales especializadas en cine y producción audiovisual. También integramos APIs de plataformas de alquiler de locaciones y bases de datos de atrezzo para acceder a información actualizada y detallada.\n",
    "\n",
    "**Almacenamiento de Datos:** (A trabajar la próxima semana)\n",
    "\n",
    "La información recopilada mediante web scraping y APIs se almacenará tanto en una base de datos relacional SQL como en una base de datos no relacional MongoDB . Estas base de datos nos permite organizar eficientemente la información sobre locaciones, atrezzo, clientes y proyectos en curso, facilitando su acceso y gestión.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Nuestro objetivo principal es proporcionar a nuestros clientes una experiencia fluida y personalizada en la búsqueda y selección de locaciones y atrezzo para sus proyectos audiovisuales. Utilizando tecnologías avanzadas y una amplia red de contactos en la industria, nos esforzamos por ofrecer soluciones creativas y de alta calidad que satisfagan las necesidades específicas de cada producción.\n",
    "\n",
    "\n",
    "## Lab: Extracción de Información con Beautiful Soup\n",
    "\n",
    "En este laboratorio seguirás enriqueciendo la base de datos para ofrecer a tus clientes un servicio más completo y personalizado. Para lograr esto, extraerás información valiosa sobre objetos de atrezzo de una web especializada. Utilizarás técnicas de web scraping con la biblioteca **Beautiful Soup** para obtener y organizar los datos de manera eficiente.\n",
    "\n",
    "Trabajarás con la siguiente URL: [Atrezzo Vázquez](https://atrezzovazquez.es/shop.php?search_type=-1&search_terms=&limit=48&page=1). Esta página contiene una gran cantidad de objetos de atrezzo que pueden ser de interés para un proyecto de rodaje. Tu tarea será extraer información relevante de los productos listados en las primeras 100 páginas.\n",
    "\n",
    "1. **Navegación y Extracción de Múltiples Páginas**:\n",
    "\n",
    "   - La página web contiene varios elementos distribuidos en distintas páginas. Tu objetivo será iterar a través de las 100 primeras páginas y extraer la información deseada.\n",
    "\n",
    "   - Debes asegurarte de que tu código sea capaz de navegar automáticamente por estas páginas y extraer datos de manera continua.\n",
    "      ```python\n",
    "      for pagina in range(1, 3):\n",
    "        url_ebay = f\"https://www.ebay.es/e/_moda/adidassneakerses?_pgn={pagina}\"\n",
    "        res_zapatillas = requests.get(url_ebay)\n",
    "        print(res_zapatillas.status_code)\n",
    "\n",
    "      # sacamos nombre zapatillas\n",
    "      sopa_zapatillas = BeautifulSoup(res_zapatillas.content, \"html.parser\")\n",
    "      lista_producto_zapatillas = sopa_zapatillas.findAll(\"h3\", {\"class\": \"s-item__title\"} )\n",
    "      if len(lista_producto_zapatillas) == 0:\n",
    "              break\n",
    "      else:\n",
    "          nombre_zapatillas = [zapatilla.getText() for zapatilla in lista_producto_zapatillas]\n",
    "          \n",
    "          # sacamos imagen zapatillas\n",
    "          lista_imagen_zapatilla = sopa_zapatillas.findAll(\"img\", {\"class\": \"s-item__image-img\"})\n",
    "          lista_imagenes = [link.get(\"data-high-res-src\")for link in lista_imagen_zapatilla]\n",
    "\n",
    "          # sacamos precio zapatillas\n",
    "          lista_precio_zapatillas = sopa_zapatillas.findAll(\"span\", {\"class\": \"s-item__price\"})\n",
    "          precio_zapatillas = [precio.getText() for precio in lista_precio_zapatillas]\n",
    "\n",
    "          df_zapatillas = pd.DataFrame({\"nombre_zapatillas\": nombre_zapatillas, \"imagen\": lista_imagenes, \"precio\": precio_zapatillas})\n",
    "          df_final = pd.concat([df_final, df_zapatillas])\n",
    "      ```\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías que necesitamos\n",
    "\n",
    "# Librerías de extracción de datos\n",
    "# -----------------------------------------------------------------------\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Tratamiento de datos\n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_atrezo = \"https://atrezzovazquez.es/shop.php?search_type=-1&search_terms=&limit=48&page=1\"\n",
    "res_atrezo = requests.get(url_atrezo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Verificación del Código de Estado de la Respuesta**:\n",
    "\n",
    "   - Antes de extraer cualquier información, es fundamental verificar que la solicitud a la página web ha sido exitosa. Un código de estado 200 indica que la página se ha cargado correctamente.\n",
    "\n",
    "   - Si el código no es 200, debes imprimir un mensaje de error y detener la ejecución de la extracción para evitar problemas posteriores.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(res_atrezo.status_code)\n",
    "sopa_atrezo = BeautifulSoup(res_atrezo.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Extracción de Información Específica**:\n",
    "\n",
    "   De cada página, deberás extraer los siguientes detalles de los objetos de atrezzo:\n",
    "\n",
    "   - **Nombre del Objeto**: El nombre o identificador del objeto.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALF1', 'ADO1']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_nombre_atrezo = sopa_atrezo.findAll(\"a\",{\"class\":\"title\"})\n",
    "nombre_atrezo = [nombre.getText() for nombre in lista_nombre_atrezo]\n",
    "nombre_atrezo[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **Categoría**: La categoría en la que se clasifica el objeto (ej.: mobiliario, decorado, utilería, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_categoria_atrezo = sopa_atrezo.findAll(\"a\",{\"class\":\"tag\"})\n",
    "categoria_atrezo = [categoria.getText() for categoria in lista_categoria_atrezo]\n",
    "categoria_df = pd.DataFrame(categoria_atrezo, columns=[\"Categoría\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   - **Sección**: La sección específica dentro de la categoría.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_seccion_atrezo = sopa_atrezo.findAll(\"div\",{\"class\":\"cat-sec-box\"})\n",
    "seccion_atrezo = [seccion.getText() for seccion in lista_seccion_atrezo]\n",
    "seccion_df = pd.DataFrame(seccion_atrezo,columns=[\"Sección 1\"])\n",
    "#\\xa0 es el &nbsp; de HTML, que es un espacio, además separamos por dos porque hay dos espacios\n",
    "seccion_df = seccion_df[\"Sección 1\"].str.split(\"\\xa0\\xa0\",expand=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **Descripción**: Una breve descripción del objeto que puede incluir detalles sobre su estilo, material o uso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_descripcion_atrezo = sopa_atrezo.findAll(\"div\",{\"class\":\"article-container style-1\"})\n",
    "descripcion_atrezo = [descripcion.getText() for descripcion in lista_descripcion_atrezo]\n",
    "descripcion_df = pd.DataFrame(descripcion_atrezo,columns=[\"Descripción\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   - **Dimensiones**: El tamaño del objeto en formato (largo x ancho x alto).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dimensiones_atrezo = sopa_atrezo.findAll(\"div\",{\"class\":\"price\"})\n",
    "dimensiones_atrezo = [dimension.getText() for dimension in lista_dimensiones_atrezo]\n",
    "dimensiones_df = pd.DataFrame(dimensiones_atrezo,columns=[\"Dimensiones\"])\n",
    "dimensiones_df = dimensiones_df[\"Dimensiones\"].str.split(\"x\",expand=True)\n",
    "dimensiones_df = dimensiones_df.rename(columns={0:\"Largo (cm)\",1:\"Ancho (cm)\",2:\"Alto (cm)\"})\n",
    "dimensiones_df[\"Largo (cm)\"] = dimensiones_df[\"Largo (cm)\"].apply(lambda x: x.replace(\"\\n\",\"\"))\n",
    "dimensiones_df[\"Alto (cm)\"] = dimensiones_df[\"Alto (cm)\"].apply(lambda x: x.replace(\"\\n\",\"\").replace(\"(cm)\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - **Enlace a la Imagen**: El link a la imagen del objeto, útil para tener una vista previa visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_imagen_atrezo = sopa_atrezo.findAll(\"h3\",{\"class\":\"s-item__title\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. **Organización de los Datos en un Diccionario**:\n",
    "\n",
    "   Una vez extraída la información, deberás organizarla en un diccionario con las siguientes claves:\n",
    "\n",
    "   - `\"nombre\"`: Nombres del objeto.\n",
    "\n",
    "   - `\"categoria\"`: Categoría a la que pertenece el objeto.\n",
    "\n",
    "   - `\"seccion\"`: Sección específica dentro de la categoría.\n",
    "\n",
    "   - `\"descripcion\"`: Breve descripción del objeto.\n",
    "\n",
    "   - `\"dimensiones\"`: Dimensiones del objeto en el formato adecuado.\n",
    "\n",
    "   - `\"imagen\"`: URL de la imagen del objeto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Almacenamiento de la Información en un DataFrame**:\n",
    "\n",
    "   Una vez que tengas toda la información organizada en diccionarios, el siguiente paso es convertirla en un DataFrame de Pandas. Este DataFrame te permitirá manipular y analizar los datos con facilidad. Tu DataFrame debería verse similar al ejemplo proporcionado, donde cada fila representa un objeto diferente y cada columna contiene la información extraída:\n",
    "\n",
    "![Dataframe](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/BS/df_atrezzo.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6.  Consideraciones Adicionales:\n",
    "\n",
    "- Asegúrate de manejar posibles errores o excepciones durante el scraping, como tiempos de espera agotados, páginas inaccesibles o datos faltantes.\n",
    "\n",
    "- Recuerda que el scraping debe hacerse de manera respetuosa, evitando sobrecargar el servidor de la página web (puedes usar pausas entre solicitudes).\n",
    "\n",
    "- Finalmente, asegúrate de almacenar el DataFrame en un archivo CSV para que puedas reutilizar esta información en análisis futuros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
